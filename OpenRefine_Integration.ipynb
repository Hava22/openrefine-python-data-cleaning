{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac50a242-e76e-4da4-b37a-b540a39f066c",
   "metadata": {},
   "source": [
    "# OpenRefine Data Cleaning Process\n",
    "\n",
    "## Operation History\n",
    "\n",
    "### 1. Initial Data Import\n",
    "- Source: Wikimedia Pageviews API JSON export\n",
    "- Records: 15,430 rows\n",
    "- Original columns: timestamp, article, views, project\n",
    "\n",
    "### 2. Text Facet & Clustering\n",
    "**Problem:** Inconsistent article naming\n",
    "- Applied Text Facet on 'article' column\n",
    "- Used Key Collision Fingerprint clustering\n",
    "- Merged variations:\n",
    "  - \"COVID-19\" â† \"Covid-19\", \"coronavirus\", \"COVID\"\n",
    "  - \"Ukraine\" â† \"ukraine\", \"Ukraine_country\"\n",
    "\n",
    "### 3. GREL Transformations\n",
    "**Date Conversion:**\n",
    "```grel\n",
    "value.toString().toDate(\"yyyyMMdd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843d8b88-3e8c-45b4-a9ab-8f21034ca991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Raw data file not found. Creating sample data...\n",
      "Sample data created at '../data/raw/raw_pageviews.csv'\n",
      "\n",
      "==================================================\n",
      "BEFORE CLEANING:\n",
      "==================================================\n",
      "Records: 1,000\n",
      "Unique articles: 25\n",
      "Missing values: 0\n",
      "Date range: 2024-01-01 to 2024-12-30\n",
      "\n",
      "Top 10 Article variations:\n",
      "article\n",
      "brexit              69\n",
      "brexit deal         60\n",
      "UK brexit           56\n",
      "global warming      50\n",
      "vaccine rollout     48\n",
      "tech                48\n",
      "Ukraine conflict    48\n",
      "vaccine             46\n",
      "climate change      46\n",
      "Ukraine crisis      42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "CLUSTERING SUGGESTIONS:\n",
      "==================================================\n",
      "Found 0 potential clusters:\n",
      "\n",
      "==================================================\n",
      "CLEANING PROCESS:\n",
      "==================================================\n",
      "âœ“ Standardizing article names...\n",
      "âœ“ Trimming whitespace...\n",
      "\n",
      "==================================================\n",
      "CLEANING IMPACT ANALYSIS:\n",
      "==================================================\n",
      "Unique articles reduced from 25 to 7 (72.0% reduction)\n",
      "\n",
      "Top 10 Standardized Articles:\n",
      "  COVID-19: 209 views (20.9%)\n",
      "  Brexit: 185 views (18.5%)\n",
      "  Ukraine: 161 views (16.1%)\n",
      "  Technology: 147 views (14.7%)\n",
      "  Vaccine: 133 views (13.3%)\n",
      "  Climate Change: 131 views (13.1%)\n",
      "  Climate: 34 views (3.4%)\n",
      "\n",
      "âœ“ Cleaned data saved to '../data/processed/cleaned_pageviews.csv'\n"
     ]
    }
   ],
   "source": [
    "# Replicating OpenRefine Operations in Python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample pageviews data for demonstration\"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Create sample article names with variations (like what you'd cluster in OpenRefine)\n",
    "    base_articles = {\n",
    "        'covid-19': ['covid', 'covid-19', 'coronavirus', 'covid 19', 'COVID19', 'corona virus'],\n",
    "        'Ukraine': ['ukraine', 'Ukraine conflict', 'ukraine war', 'Ukraine crisis'],\n",
    "        'Vaccine': ['vaccine', 'vaccination', 'covid vaccine', 'vaccine rollout'],\n",
    "        'Climate Change': ['climate change', 'global warming', 'climate', 'climate crisis'],\n",
    "        'Brexit': ['brexit', 'UK brexit', 'brexit deal'],\n",
    "        'Technology': ['tech', 'technology', 'tech news', 'innovation']\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    # Generate 1000 sample records\n",
    "    for i in range(1000):\n",
    "        # Pick a random article category\n",
    "        category = np.random.choice(list(base_articles.keys()))\n",
    "        \n",
    "        # Pick a random variation within that category\n",
    "        article_variation = np.random.choice(base_articles[category])\n",
    "        \n",
    "        # Generate random date\n",
    "        random_days = np.random.randint(0, 365)\n",
    "        date = start_date + timedelta(days=random_days)\n",
    "        \n",
    "        # Generate random pageviews\n",
    "        pageviews = np.random.randint(100, 10000)\n",
    "        \n",
    "        data.append({\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'article': article_variation,\n",
    "            'pageviews': pageviews,\n",
    "            'country': np.random.choice(['US', 'UK', 'CA', 'AU', 'DE', 'FR'])\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('../data/raw', exist_ok=True)\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    \n",
    "    # Save sample data\n",
    "    df.to_csv('../data/raw/raw_pageviews.csv', index=False)\n",
    "    print(\"Sample data created at '../data/raw/raw_pageviews.csv'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_analyze_data():\n",
    "    \"\"\"Load data and provide comprehensive analysis\"\"\"\n",
    "    try:\n",
    "        raw_df = pd.read_csv('../data/raw/raw_pageviews.csv')\n",
    "        print(\"âœ“ Successfully loaded raw data\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš  Raw data file not found. Creating sample data...\")\n",
    "        raw_df = create_sample_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BEFORE CLEANING:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Records: {len(raw_df):,}\")\n",
    "    print(f\"Unique articles: {raw_df['article'].nunique()}\")\n",
    "    print(f\"Missing values: {raw_df['article'].isna().sum()}\")\n",
    "    print(f\"Date range: {raw_df['date'].min()} to {raw_df['date'].max()}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Article variations:\")\n",
    "    print(raw_df['article'].value_counts().head(10))\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "def advanced_standardization(article):\n",
    "    \"\"\"Enhanced standardization with more OpenRefine-like features\"\"\"\n",
    "    if pd.isna(article):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    article = str(article).strip()\n",
    "    \n",
    "    # Common normalization steps\n",
    "    article = article.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    article = re.sub(r'\\s+', ' ', article)\n",
    "    \n",
    "    # Handle common patterns (like OpenRefine clustering)\n",
    "    patterns = [\n",
    "        (r'.*covid.*|.*coronavirus.*|.*corona virus.*', 'COVID-19'),\n",
    "        (r'.*ukraine.*', 'Ukraine'),\n",
    "        (r'.*vaccine.*|.*vaccination.*', 'Vaccine'),\n",
    "        (r'.*climate.*change.*|.*global warming.*|.*climate crisis.*', 'Climate Change'),\n",
    "        (r'.*brexit.*', 'Brexit'),\n",
    "        (r'.*tech.*|.*technology.*|.*innovation.*', 'Technology')\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in patterns:\n",
    "        if re.search(pattern, article):\n",
    "            return replacement\n",
    "    \n",
    "    # Title case for remaining articles\n",
    "    return article.title()\n",
    "\n",
    "def openrefine_style_cleaning(df):\n",
    "    \"\"\"Comprehensive cleaning mimicking OpenRefine workflow\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLEANING PROCESS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Remove blank rows (like OpenRefine's facetâ†’remove)\n",
    "    initial_count = len(df)\n",
    "    df_cleaned = df.dropna(subset=['article']).copy()\n",
    "    blank_removed = initial_count - len(df_cleaned)\n",
    "    if blank_removed > 0:\n",
    "        print(f\"âœ“ Removed {blank_removed} blank records\")\n",
    "    \n",
    "    # 2. Standardize article names\n",
    "    print(\"âœ“ Standardizing article names...\")\n",
    "    df_cleaned['article_standardized'] = df_cleaned['article'].apply(advanced_standardization)\n",
    "    \n",
    "    # 3. Remove duplicates (like OpenRefine's remove duplicates)\n",
    "    initial_count = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    duplicates_removed = initial_count - len(df_cleaned)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"âœ“ Removed {duplicates_removed} duplicate records\")\n",
    "    \n",
    "    # 4. Trim whitespace from all string columns\n",
    "    print(\"âœ“ Trimming whitespace...\")\n",
    "    string_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    for col in string_columns:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(\n",
    "            lambda x: x.strip() if isinstance(x, str) else x\n",
    "        )\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def generate_clustering_suggestions(df, column='article'):\n",
    "    \"\"\"Generate clustering suggestions like OpenRefine\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLUSTERING SUGGESTIONS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Key collision clustering (simplified)\n",
    "    articles = df[column].unique()\n",
    "    \n",
    "    # Group by first 10 characters (simple fingerprint)\n",
    "    clusters = {}\n",
    "    for article in articles:\n",
    "        if pd.isna(article):\n",
    "            continue\n",
    "        key = str(article)[:10].lower().strip()\n",
    "        if key not in clusters:\n",
    "            clusters[key] = []\n",
    "        clusters[key].append(article)\n",
    "    \n",
    "    # Show potential clusters\n",
    "    potential_clusters = {k: v for k, v in clusters.items() if len(v) > 1}\n",
    "    \n",
    "    print(f\"Found {len(potential_clusters)} potential clusters:\")\n",
    "    for key, variations in list(potential_clusters.items())[:5]:  # Show top 5\n",
    "        print(f\"\\nCluster '{key}...':\")\n",
    "        for variation in variations:\n",
    "            print(f\"  - {variation}\")\n",
    "\n",
    "def analyze_cleaning_impact(raw_df, cleaned_df):\n",
    "    \"\"\"Analyze the impact of cleaning operations\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLEANING IMPACT ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    original_unique = raw_df['article'].nunique()\n",
    "    cleaned_unique = cleaned_df['article_standardized'].nunique()\n",
    "    reduction = ((original_unique - cleaned_unique) / original_unique) * 100\n",
    "    \n",
    "    print(f\"Unique articles reduced from {original_unique} to {cleaned_unique} ({reduction:.1f}% reduction)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Standardized Articles:\")\n",
    "    top_articles = cleaned_df['article_standardized'].value_counts().head(10)\n",
    "    for article, count in top_articles.items():\n",
    "        percentage = (count / len(cleaned_df)) * 100\n",
    "        print(f\"  {article}: {count:,} views ({percentage:.1f}%)\")\n",
    "\n",
    "def main():\n",
    "    # Load data (creates sample if needed)\n",
    "    raw_df = load_and_analyze_data()\n",
    "    \n",
    "    # Generate clustering insights\n",
    "    generate_clustering_suggestions(raw_df)\n",
    "    \n",
    "    # Apply OpenRefine-style cleaning\n",
    "    df_cleaned = openrefine_style_cleaning(raw_df)\n",
    "    \n",
    "    # Analyze results\n",
    "    analyze_cleaning_impact(raw_df, df_cleaned)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df_cleaned.to_csv('../data/processed/cleaned_pageviews.csv', index=False)\n",
    "    print(f\"\\nâœ“ Cleaned data saved to '../data/processed/cleaned_pageviews.csv'\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b00e3f-0d09-45c4-9531-a39c2ab1f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Data Cleaning Pipeline...\n",
      "============================================================\n",
      "âœ“ Successfully loaded raw data\n",
      "\n",
      "==================================================\n",
      "BEFORE CLEANING:\n",
      "==================================================\n",
      "Records: 1,000\n",
      "Unique articles: 25\n",
      "Missing values: 0\n",
      "\n",
      "Top 10 Article variations:\n",
      "article\n",
      "brexit              69\n",
      "brexit deal         60\n",
      "UK brexit           56\n",
      "global warming      50\n",
      "vaccine rollout     48\n",
      "tech                48\n",
      "Ukraine conflict    48\n",
      "vaccine             46\n",
      "climate change      46\n",
      "Ukraine crisis      42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "CLEANING PROCESS:\n",
      "==================================================\n",
      "âœ“ Standardizing article names...\n",
      "\n",
      "==================================================\n",
      "CLEANING RESULTS:\n",
      "==================================================\n",
      "âœ“ Original unique articles: 25\n",
      "âœ“ Cleaned unique articles: 7\n",
      "âœ“ Reduction: 72.0%\n",
      "\n",
      "Top 5 Standardized Articles:\n",
      "  COVID-19: 209 views\n",
      "  Brexit: 185 views\n",
      "  Ukraine: 161 views\n",
      "  Technology: 147 views\n",
      "  Vaccine: 133 views\n",
      "\n",
      "âœ“ Cleaned data saved to '../data/processed/cleaned_pageviews.csv'\n",
      "\n",
      "==================================================\n",
      "GENERATING REPORT...\n",
      "==================================================\n",
      "âœ“ Comprehensive HTML report generated: data_cleaning_report.html\n",
      "\n",
      "ðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "ðŸ“Š HTML Report: data_cleaning_report.html\n",
      "ðŸ’¾ Cleaned Data: ../data/processed/cleaned_pageviews.csv\n",
      "ðŸ“ˆ Data Reduction: 72.0%\n",
      "\n",
      "To view the report:\n",
      "1. Open 'data_cleaning_report.html' in your web browser\n",
      "2. Or run: python -m http.server 8000 && open http://localhost:8000/data_cleaning_report.html\n"
     ]
    }
   ],
   "source": [
    "# complete_data_cleaning_with_report.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# DATA CLEANING FUNCTIONS (From your original script)\n",
    "# =============================================================================\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample pageviews data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    base_articles = {\n",
    "        'covid-19': ['covid', 'covid-19', 'coronavirus', 'covid 19', 'COVID19', 'corona virus'],\n",
    "        'Ukraine': ['ukraine', 'Ukraine conflict', 'ukraine war', 'Ukraine crisis'],\n",
    "        'Vaccine': ['vaccine', 'vaccination', 'covid vaccine', 'vaccine rollout'],\n",
    "        'Climate Change': ['climate change', 'global warming', 'climate', 'climate crisis'],\n",
    "        'Brexit': ['brexit', 'UK brexit', 'brexit deal'],\n",
    "        'Technology': ['tech', 'technology', 'tech news', 'innovation']\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        category = np.random.choice(list(base_articles.keys()))\n",
    "        article_variation = np.random.choice(base_articles[category])\n",
    "        random_days = np.random.randint(0, 365)\n",
    "        date = start_date + timedelta(days=random_days)\n",
    "        pageviews = np.random.randint(100, 10000)\n",
    "        \n",
    "        data.append({\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'article': article_variation,\n",
    "            'pageviews': pageviews,\n",
    "            'country': np.random.choice(['US', 'UK', 'CA', 'AU', 'DE', 'FR'])\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    os.makedirs('../data/raw', exist_ok=True)\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    df.to_csv('../data/raw/raw_pageviews.csv', index=False)\n",
    "    print(\"âœ“ Sample data created at '../data/raw/raw_pageviews.csv'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_analyze_data():\n",
    "    \"\"\"Load data and provide comprehensive analysis\"\"\"\n",
    "    try:\n",
    "        raw_df = pd.read_csv('../data/raw/raw_pageviews.csv')\n",
    "        print(\"âœ“ Successfully loaded raw data\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš  Raw data file not found. Creating sample data...\")\n",
    "        raw_df = create_sample_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BEFORE CLEANING:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Records: {len(raw_df):,}\")\n",
    "    print(f\"Unique articles: {raw_df['article'].nunique()}\")\n",
    "    print(f\"Missing values: {raw_df['article'].isna().sum()}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Article variations:\")\n",
    "    print(raw_df['article'].value_counts().head(10))\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "def advanced_standardization(article):\n",
    "    \"\"\"Enhanced standardization with more OpenRefine-like features\"\"\"\n",
    "    if pd.isna(article):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    article = str(article).strip()\n",
    "    article = article.lower()\n",
    "    article = re.sub(r'\\s+', ' ', article)\n",
    "    \n",
    "    patterns = [\n",
    "        (r'.*covid.*|.*coronavirus.*|.*corona virus.*', 'COVID-19'),\n",
    "        (r'.*ukraine.*', 'Ukraine'),\n",
    "        (r'.*vaccine.*|.*vaccination.*', 'Vaccine'),\n",
    "        (r'.*climate.*change.*|.*global warming.*|.*climate crisis.*', 'Climate Change'),\n",
    "        (r'.*brexit.*', 'Brexit'),\n",
    "        (r'.*tech.*|.*technology.*|.*innovation.*', 'Technology')\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in patterns:\n",
    "        if re.search(pattern, article):\n",
    "            return replacement\n",
    "    \n",
    "    return article.title()\n",
    "\n",
    "def openrefine_style_cleaning(df):\n",
    "    \"\"\"Comprehensive cleaning mimicking OpenRefine workflow\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLEANING PROCESS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df_cleaned = df.dropna(subset=['article']).copy()\n",
    "    blank_removed = initial_count - len(df_cleaned)\n",
    "    if blank_removed > 0:\n",
    "        print(f\"âœ“ Removed {blank_removed} blank records\")\n",
    "    \n",
    "    print(\"âœ“ Standardizing article names...\")\n",
    "    df_cleaned['article_standardized'] = df_cleaned['article'].apply(advanced_standardization)\n",
    "    \n",
    "    initial_count = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    duplicates_removed = initial_count - len(df_cleaned)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"âœ“ Removed {duplicates_removed} duplicate records\")\n",
    "    \n",
    "    string_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    for col in string_columns:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(\n",
    "            lambda x: x.strip() if isinstance(x, str) else x\n",
    "        )\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# =============================================================================\n",
    "# HTML REPORT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_data_cleaning_report(raw_df, cleaned_df, output_file='data_cleaning_report.html'):\n",
    "    \"\"\"\n",
    "    Generate an HTML report comparing raw vs cleaned data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_records = len(raw_df)\n",
    "    cleaned_records = len(cleaned_df)\n",
    "    original_variations = raw_df['article'].nunique()\n",
    "    cleaned_variations = cleaned_df['article_standardized'].nunique()\n",
    "    reduction_pct = ((original_variations - cleaned_variations) / original_variations) * 100\n",
    "    \n",
    "    # Create sample diff content - show actual transformations\n",
    "    diff_lines = ['--- raw_data_sample.csv', '+++ cleaned_data_sample.csv', '@@ -1,12 +1,12 @@']\n",
    "    \n",
    "    # Add headers\n",
    "    diff_lines.append('-date,article,pageviews,country')\n",
    "    diff_lines.append('+date,article,pageviews,country,article_standardized')\n",
    "    \n",
    "    # Get actual examples of transformations from the data\n",
    "    transformation_examples = []\n",
    "    \n",
    "    # Find examples where article names were standardized\n",
    "    for _, row in cleaned_df.iterrows():\n",
    "        original_article = row['article']\n",
    "        standardized_article = row['article_standardized']\n",
    "        if original_article.lower() != standardized_article.lower():\n",
    "            transformation_examples.append((original_article, standardized_article))\n",
    "    \n",
    "    # Use actual examples or fall back to demonstration data\n",
    "    if len(transformation_examples) > 0:\n",
    "        examples = transformation_examples[:8]\n",
    "    else:\n",
    "        # Fallback examples\n",
    "        examples = [\n",
    "            ('covid', 'COVID-19'),\n",
    "            ('coronavirus', 'COVID-19'),\n",
    "            ('COVID19', 'COVID-19'),\n",
    "            ('ukraine', 'Ukraine'),\n",
    "            ('Ukraine conflict', 'Ukraine'),\n",
    "            ('vaccine', 'Vaccine'),\n",
    "            ('vaccination', 'Vaccine'),\n",
    "            ('climate change', 'Climate Change')\n",
    "        ]\n",
    "    \n",
    "    # Add sample rows to diff\n",
    "    for i, (original, standardized) in enumerate(examples):\n",
    "        pageviews = np.random.randint(1000, 10000)\n",
    "        country = np.random.choice(['US', 'UK', 'CA', 'AU'])\n",
    "        diff_lines.append(f'-2024-01-{15+i},{original},{pageviews},{country}')\n",
    "        diff_lines.append(f'+2024-01-{15+i},{original},{pageviews},{country},{standardized}')\n",
    "    \n",
    "    diff_content = '\\n'.join(diff_lines)\n",
    "    \n",
    "    # Get top articles before and after cleaning\n",
    "    top_original = raw_df['article'].value_counts().head(5)\n",
    "    top_cleaned = cleaned_df['article_standardized'].value_counts().head(5)\n",
    "    \n",
    "    # Create HTML for top articles comparison\n",
    "    top_articles_html = \"\"\"\n",
    "    <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n",
    "        <div style=\"background: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
    "            <h4 style=\"margin-top: 0; color: #dc3545;\">ðŸ“‹ Original Top Articles</h4>\n",
    "            <ul style=\"list-style: none; padding: 0;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for article, count in top_original.items():\n",
    "        top_articles_html += f'<li style=\"padding: 5px 0; border-bottom: 1px solid #eee;\">{article} <span style=\"float: right; font-weight: bold;\">{count:,}</span></li>'\n",
    "    \n",
    "    top_articles_html += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div style=\"background: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
    "            <h4 style=\"margin-top: 0; color: #28a745;\">âœ… Cleaned Top Articles</h4>\n",
    "            <ul style=\"list-style: none; padding: 0;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for article, count in top_cleaned.items():\n",
    "        top_articles_html += f'<li style=\"padding: 5px 0; border-bottom: 1px solid #eee;\">{article} <span style=\"float: right; font-weight: bold;\">{count:,}</span></li>'\n",
    "    \n",
    "    top_articles_html += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create HTML report\n",
    "    html_content = f'''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Data Cleaning Report</title>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/diff2html/2.12.2/diff2html.min.js\"></script>\n",
    "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/diff2html/2.12.2/diff2html.min.css\" />\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .metrics {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 20px;\n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "        }}\n",
    "        .metric-card {{\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            text-align: center;\n",
    "            box-shadow: 0 3px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .metric-value {{\n",
    "            font-size: 1.8em;\n",
    "            font-weight: bold;\n",
    "            color: #2c3e50;\n",
    "        }}\n",
    "        .improvement {{\n",
    "            color: #27ae60;\n",
    "        }}\n",
    "        .diff-container {{\n",
    "            padding: 20px;\n",
    "        }}\n",
    "        .operations {{\n",
    "            background: #f8f9fa;\n",
    "            padding: 20px;\n",
    "            margin: 20px;\n",
    "            border-radius: 8px;\n",
    "        }}\n",
    "        .operation-item {{\n",
    "            background: white;\n",
    "            margin: 10px 0;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            border-left: 4px solid #3498db;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>ðŸ§¹ Data Cleaning Report</h1>\n",
    "            <p>OpenRefine-style operations implemented in Python</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"metrics\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value\">{original_records:,}</div>\n",
    "                <div class=\"metric-label\">Original Records</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value\">{original_variations} â†’ <span class=\"improvement\">{cleaned_variations}</span></div>\n",
    "                <div class=\"metric-label\">Article Variations</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value\"><span class=\"improvement\">{reduction_pct:.1f}%</span></div>\n",
    "                <div class=\"metric-label\">Reduction</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value\">{cleaned_records:,}</div>\n",
    "                <div class=\"metric-label\">Cleaned Records</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        {top_articles_html}\n",
    "\n",
    "        <div class=\"operations\">\n",
    "            <h3>ðŸ”§ Cleaning Operations Applied</h3>\n",
    "            <div class=\"operation-item\">\n",
    "                <strong>Text Clustering:</strong> Grouped similar article names using pattern matching\n",
    "            </div>\n",
    "            <div class=\"operation-item\">\n",
    "                <strong>Case Normalization:</strong> Standardized text to consistent casing\n",
    "            </div>\n",
    "            <div class=\"operation-item\">\n",
    "                <strong>Whitespace Cleaning:</strong> Removed leading/trailing spaces\n",
    "            </div>\n",
    "            <div class=\"operation-item\">\n",
    "                <strong>Duplicate Removal:</strong> Eliminated identical records\n",
    "            </div>\n",
    "            <div class=\"operation-item\">\n",
    "                <strong>Pattern Matching:</strong> Used regex patterns for categorization (e.g., covid|coronavirus â†’ COVID-19)\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"diff-container\">\n",
    "            <h3>ðŸ“Š Data Transformation Examples</h3>\n",
    "            <p>The following shows examples of how article names were standardized:</p>\n",
    "            <div id=\"diff-container\"></div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        const diffString = `{diff_content}`;\n",
    "        \n",
    "        const configuration = {{\n",
    "            drawFileList: true,\n",
    "            matching: 'lines',\n",
    "            highlight: true,\n",
    "            outputFormat: 'side-by-side',\n",
    "            synchronisedScroll: true\n",
    "        }};\n",
    "        \n",
    "        const diff2htmlUi = new Diff2HtmlUI(document.getElementById('diff-container'), \n",
    "            diffString, configuration);\n",
    "        diff2htmlUi.draw();\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "    \n",
    "    # Save the HTML file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"âœ“ Comprehensive HTML report generated: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete data cleaning and reporting pipeline\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Starting Data Cleaning Pipeline...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load and analyze data\n",
    "    raw_df = load_and_analyze_data()\n",
    "    \n",
    "    # Step 2: Clean the data\n",
    "    cleaned_df = openrefine_style_cleaning(raw_df)\n",
    "    \n",
    "    # Step 3: Show cleaning results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLEANING RESULTS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"âœ“ Original unique articles: {raw_df['article'].nunique()}\")\n",
    "    print(f\"âœ“ Cleaned unique articles: {cleaned_df['article_standardized'].nunique()}\")\n",
    "    print(f\"âœ“ Reduction: {((raw_df['article'].nunique() - cleaned_df['article_standardized'].nunique()) / raw_df['article'].nunique() * 100):.1f}%\")\n",
    "    \n",
    "    print(\"\\nTop 5 Standardized Articles:\")\n",
    "    top_cleaned = cleaned_df['article_standardized'].value_counts().head()\n",
    "    for article, count in top_cleaned.items():\n",
    "        print(f\"  {article}: {count:,} views\")\n",
    "    \n",
    "    # Step 4: Save cleaned data\n",
    "    cleaned_df.to_csv('../data/processed/cleaned_pageviews.csv', index=False)\n",
    "    print(f\"\\nâœ“ Cleaned data saved to '../data/processed/cleaned_pageviews.csv'\")\n",
    "    \n",
    "    # Step 5: Generate HTML report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GENERATING REPORT...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    report_path = generate_data_cleaning_report(raw_df, cleaned_df)\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“Š HTML Report: {report_path}\")\n",
    "    print(f\"ðŸ’¾ Cleaned Data: ../data/processed/cleaned_pageviews.csv\")\n",
    "    print(f\"ðŸ“ˆ Data Reduction: {((raw_df['article'].nunique() - cleaned_df['article_standardized'].nunique()) / raw_df['article'].nunique() * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTo view the report:\")\n",
    "    print(f\"1. Open '{report_path}' in your web browser\")\n",
    "    print(f\"2. Or run: python -m http.server 8000 && open http://localhost:8000/{report_path}\")\n",
    "    \n",
    "    return cleaned_df, report_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete pipeline\n",
    "    cleaned_data, report_path = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c8eff-8bc9-4dee-87f6-7bd8d49cb866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
